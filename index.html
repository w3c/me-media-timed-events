<!DOCTYPE html>
<html>
  <head>
    <title>Media Timed Events</title>
    <meta charset="utf-8">
    <script src="https://www.w3.org/Tools/respec/respec-w3c-common" async class="remove"></script>
    <script class="remove">
      var respecConfig = {
        specStatus: "ED",
        edDraftURI: "https://w3c.github.io/me-media-timed-events/",
        shortName: "media-timed-events",
        editors: [
          {
            name: "Chris Needham",
            mailto: "chris.needham@bbc.co.uk",
            company: "British Broadcasting Corporation",
            companyURL: "https://www.bbc.co.uk"
          },
          {
            name: "Giridhar Mandyam",
            mailto: "mandyam@qti.qualcomm.com",
            company: "Qualcomm",
            companyURL: "https://www.qualcomm.com"
          }
        ],
        wg: "Media & Entertainment Interest Group",
        wgURI: "https://www.w3.org/2011/webtv/",
        charterDisclosureURI: "https://www.w3.org/2017/03/webtv-charter.html",
        wgPublicList: "public-web-and-tv",
        localBiblio: {
          "WEB-ISOBMFF": {
            title: "ISO/IEC JTC1/SC29/WG11 N16944 Working Draft on Carriage of Web Resources in ISOBMFF",
            href: "https://mpeg.chiariglione.org/standards/mpeg-4/timed-text-and-other-visual-overlays-iso-base-media-file-format/wd-carriage-web",
            // href: "https://mpeg.chiariglione.org/sites/default/files/files/standards/parts/docs/w16944.zip",
            authors: [
              "Thomas Stockhammer",
              "Cyril Concolato"
            ],
            publisher: "MPEG",
            date: "July 2017",
          },
          "DASH-EVENTING": {
            title: "DASH Eventing and HTML5",
            href: "https://www.w3.org/2011/webtv/wiki/images/a/a5/DASH_Eventing_and_HTML5.pdf",
            authors: [
              "Giridhar Mandyam"
            ],
            date: "February 2018"
          },
          "WEB-MEDIA-GUIDELINES": {
            title: "Web Media Application Developer Guidelines 2018",
            href: "https://w3c.github.io/webmediaguidelines/",
            authors: [
              "Joel Korpi",
              "Thasso Griebel",
              "Jeff Burtoft"
            ],
            publisher: "W3C",
            status: "CG-DRAFT",
            date: "26 April 2018"
          },
          "HBBTV": {
            title: "HbbTV 2.0.2 Specification",
            href: "https://www.hbbtv.org/wp-content/uploads/2018/02/HbbTV_v202_specification_2018_02_16.pdf",
            publisher: "HbbTV Association",
            date: "16 February 2018"
          },
          "HBBTV-TESTS": {
            title: "HbbTV Test Suite 2018-1",
            href: "https://www.hbbtv.org/wp-content/uploads/2018/03/HbbTV-testcases-2018-1.pdf",
            publisher: "HbbTV Association",
            date: "2018"
          },
          "DVB-DASH": {
            title: "DVB Document A168. Digital Video Broadcasting (DVB); MPEG-DASH Profile for Transport of ISO BMFF Based DVB Services over IP Based Networks",
            href: "https://www.dvb.org/resources/public/standards/a168_dvb_mpeg-dash_nov_2017.pdf",
            publisher: "DVB",
            date: "November 2017"
          },
          "BBC-SUBTITLES": {
            title: "Subtitle Guidelines",
            href: "http://bbc.github.io/subtitle-guidelines/",
            publisher: "BBC",
            date: "May 2018"
          },
        }
      };
    </script>
  </head>
  <body>
    <section id="abstract">
      <p>
        This document collects use cases and requirements for improved support
        for timed events related to audio or video media on the Web, such as
        subtitles, captions, or other web content, where synchronization to a
        playing audio or video media stream is needed, and makes recommendations
        for new or changed Web APIs to realize these requirements.
      </p>
    </section>
    <section id="sotd">
    </section>
    <section>
      <h2>Introduction</h2>
      <p class="ednote">
        Add a general introduction to media timed events, what they are, what
        they're for...
      </p>
    </section>
    <section>
      <h2>Terminology</h2>
      <p>
        The following terms are used in this document:
        <ul>
          <li>
            <dfn>in-band</dfn> &mdash; timed event information that is delivered
            within the audio or video media container or multiplexed with the
            media stream.
          </li>
          <li>
            <dfn>out-of-band</dfn> &mdash; timed event information that is
            delivered over some other mechanism external to the media container
            or media stream.
          </li>
        </ul>
      </p>
    </section>
    <section>
      <h2>Use cases</h2>
      <p>
        This section describes specific use cases for media timed events.
      </p>
      <section>
        <h3>Synchronised event triggering</h3>
        <p class="ednote">
          Add use case descriptions for DASH and emsg events here.
          Describe a few motivating application scenarios.
        </p>
        <p>
          In MPEG DASH, events may be conveyed either as in-band events, e.g.,
          as <code>emsg</code> boxes in ISO BMFF files, or out-of-band, via an
          EventStream fragment in the MPD (Media Presentation Description)
          document. In addition, the MPD document may advertise the presence of
          <code>emsg</code> events in the ISO BMFF content for given schemas.
        </p>
        <p>
          Use cases for <code>emsg</code> boxes include:
          <ul>
            <li>
              Displaying of images alongside an audio stream. For example, in
              RadioVIS in DVB, the <code>emsg</code> event contains an image URL,
              which the UA requests. In this use case, synchronization of the
              image rendering to within a second or so is acceptable
              ([[DVB-DASH]], section 9.1.7).
            </li>
            <li>
              Notifying the DASH player Web application that it should refresh
              its copy of the MPD document ([[MPEGDASH]], section 5.10.4).
              This is an alternative to setting a cache duration in the HTTP
              response, so the client can refresh the MPD when it actually
              changes, and reduces the load on HTTP servers caused by frequent
              server requests.
            </li>
          </ul>
        </p>
        <p>
          Reference: M&amp;E IG call 1 Feb 2018:
          <a href="https://www.w3.org/2018/02/01-me-minutes.html">Minutes</a>,
          [[DASH-EVENTING]].
        </p>
        <p class="ednote">
          See also <a href="https://github.com/w3c/webmediaguidelines/issues/64">this issue</a>
          against the [[WEB-MEDIA-GUIDELINES]]. TODO: Add detail here.
        </p>
      </section>
      <section>
        <h3>Synchronized rendering of web resources</h3>
        <p class="ednote">
          Add use case descriptions for synchronised rendering here. Note that
          this could be rendering of any web resource, not necessarily those
          embedded in media containers. Describe a few motivating application
          scenarios.
        </p>
      </section>
      <section>
        <h3>Rendering of Web content embedded in media containers</h3>
        <p>
          Media-timed events can be used to trigger retrieval and/or rendering
          of web resources.  Such resources can be used to enhance user experience
          in the context of media that is being rendered.  Some examples include
        <ul>
          <li>
            Display of social media feeds corresponding to a live broadcast such as a sporting event.
          </li>
          <li>
            Banner advertisements for sponsored content.
          </li>
          <li>
            Accessibility-related assets, such as large print rendering of captions.
          </li>
        </ul>
        </p>
        <p class="ednote">
          Add use case descriptions for rendering of Web content embedded in
          media containers (e.g., [[WEB-ISOBMFF]]). Describe a few motivating
          application scenarios.
        </p>
      </section>
    </section>
    <section>
      <h2>Related industry standards</h2>
      <p class="ednote">
        Link to and describe relevant documents from other standards groups here:
        <ul class="ednote">
          <li>[[MPEGDASH]] events</li>
          <li>[[ISOBMFF]] <code>emsg</code></li>
          <li>etc.</li>
        </ul>
      </p>
      <section>
        <h3>MPEG Working Draft on Carriage of Web Resources in ISOBMFF</h3>
        <p>
          [[WEB-ISOBMFF]] specifies the use of ISO BMFF tools for the storage
          and delivery of web data. The specified storage is designed to enable
          enriching audio/video content, as well as audio-only content, with
          synchronized, animated, interactive web data, including overlays.
        </p>
      </section>
      <section>
        <h3>HbbTV</h3>
        <p>
          HbbTV includes support for <code>emsg</code> events ([[DVB-DASH]],
          section 9.1) and requires this be mapped to <code>DataCue</code>
          ([[HBBTV]], section 9.3.2). The HbbTV device test suite includes
          test pages and streams for this ([[HBBTV-TESTS]]). This feature is
          included in user agents shipping in connected TVs across Europe from
          2017. HbbTV has a <a href="https://github.com/HbbTV-Association/ReferenceApplication">reference app</a>
          and content for DASH+DRM which includes <code>emsg</code> support.
          As well as HbbTV devices, this reference app and content have been
          tested on Microsoft Edge with MSE and EME although <code>emsg</code>
          support does not work there.
        </p>
      </section>
      <section>
        <h3>BBC Subtitle Guidelines</h3>
        <p>
          The BBC Subtitle Guidelines ([[BBC-SUBTITLES]]) describe best practice
          for authoring subtitles or captions. In particular, the guidelines state:
        </p>
        <blockquote>
          <p>
            <b>5.2 Match subtitle to pace of speaking</b>
          </p>
          <p>
            The subtitles should match the pace of speaking as closely
            as possible. Ideally, when the speaker is in shot, your subtitles
            should not anticipate speech by more than 1.5 seconds or hang up on
            the screen for more than 1.5 seconds after speech has stopped.
          </p>
          <p>
            <b>6.1 Match subtitles to shot</b>
          </p>
          <p>
            It is likely to be less tiring for the viewer if shot changes and
            subtitle changes occur at the same time. Many subtitles therefore
            start on the first frame of the shot and end on the last frame.
          </p>
          <p>
            <b>6.2 Maintain a minimum gap when mismatched</b>
          </p>
          <p>
            If you have to let a subtitle hang over a shot change, do not remove
            it too soon after the cut. The duration of the overhang will depend
            on the content.
          </p>
          <p>
            <b>6.3 Avoid straddling shot changes</b>
          </p>
          <p>
            Avoid creating subtitles that straddle a shot change (i.e., a subtitle
            that starts in the middle of shot one and ends in the middle of shot
            two). To do this, you may need to split a sentence at an appropriate
            point, or delay the start of a new sentence to coincide with the shot
            change.
          </p>
        </blockquote>
        <p>
          To meet these requirements, the playback system must honour the provided
          timings. Subtitles for video are typically authored against video at a
          nominal frame rate, e.g., 25 frames per second (which corresponds to
          40 milliseconds per frame). The actual video frame rate may be adjusted
          dynamically according to the video encoding, but the subtitle timing
          must remain the same ([[EBU-TT-D]], Annex E).
        </p>
      </section>
    </section>
    <section>
      <h2>Gap analysis</h2>
      <p>
        This section describes gaps in existing existing Web platform
        capabilities needed to support the use cases and requirements described
        in this document. Where applicable, this section also describes how
        existing Web platform features can be used as workarounds, and any
        associated limitations.
      </p>
      <section>
        <h3>Synchronized event triggering</h3>
        <section>
          <h4>DASH and ISO BMFF emsg events</h4>
          <p>
            An <code>emsg</code> event contains the following information,
            as specified in [[MPEGDASH]], section 5.10.3.3:
            <ul>
              <li><code>scheme_id_uri</code> &mdash; A URI that identifies
              the message scheme</li>
              <li><code>value</code> &mdash; The event value (string)</li>
              <li><code>timescale</code> &mdash; Timescale units, in ticks
              per second</li>
              <li><code>presentation_time_delta</code> &mdash; Presentation
              time delta (with respect to the media segment),
              in <code>timescale</code> units</li>
              <li><code>event_duration</code> &mdash; Event duration,
              in <code>timescale</code> units</li>
              <li><code>id</code> &mdash; Event message identifier</li>
              <li><code>message_data</code> &mdash; Message body (may be empty)</li>
            </ul>
          </p>
          <p>
            The presence of <code>emsg</code> events in the media stream is signalled
            in the DASH manifest document (MPD), using an <code>EventStream</code> XML
            element ([[MPEGDASH]], section 5.10.2).
          </p>
          <p>
            The <code>DataCue</code> API has been previously discussed as a means to
            deliver in-band event data to Web applications, but this is not implemented
            in mainstream browser engines. It is <a href="https://www.w3.org/TR/2018/WD-html53-20180426/semantics-embedded-content.html#text-tracks-exposing-inband-metadata">included</a>
            in the 26 April 2018 HTML 5.3 draft [[HTML53-20180426]], but is
            <a href="https://html.spec.whatwg.org/multipage/media.html#timed-text-tracks">not included</a>
            in [[HTML]]. See discussion <a href="https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/U06zrT2N-Xk">here</a>
            and notes on implementation status <a href="https://lists.w3.org/Archives/Public/public-html/2016Apr/0005.html">here</a>.
          </p>
          <p>
            Neither [[MSE-BYTE-STREAM-FORMAT-ISOBMFF]] nor [[INBANDTRACKS]] describe
            handling of <code>emsg</code> boxes.
          </p>
          <p>
            [[HBBTV]] section 9.3.2 describes a mapping between the <code>emsg</code>
            fields described <a href="#dash-and-iso-bmff-emsg-events">above</a>
            and the <a href="https://html.spec.whatwg.org/multipage/media.html#texttrack"><code>TextTrack</code></a>
            and <a href="https://www.w3.org/TR/2018/WD-html53-20180426/semantics-embedded-content.html#datacue"><code>DataCue</code></a>
            APIs. A <code>TextTrack</code> instance is created for each event
            stream signalled in the MPD document (as identified by the
            <code>schemeIdUri</code> and <code>value</code>), and the
            <a href="https://html.spec.whatwg.org/multipage/media.html#dom-texttrack-inbandmetadatatrackdispatchtype"><code>inBandMetadataTrackDispatchType</code></a>
            <code>TextTrack</code> attribute contains the <code>scheme_id_uri</code>
            and <code>value </code> values. Because HbbTV devices include a native
            DASH client, parsing of the MPD document and creation of the
            <code>TextTrack</code>s is done by the UA.
          </p>
          <p class="ednote">
            To support DASH clients implemented in Web applications, there is
            therefore either a need for an API that allows applications to tell
            the UA which schemes it wants to receive, or the UA should simply
            expose all event streams to applications. Which of these is preferred?
          </p>
        </section>
        <section>
          <h4>Synchronization and timing</h4>
          <p>
            The timing guarantees provided in HTML5 regarding the triggering of
            <code>TextTrackCue</code> events may be not be enough to avoid
            <a href="https://lists.w3.org/Archives/Public/public-inbandtracks/2013Dec/0004.html">events being missed</a>.
          </p>
        </section>
      </section>
      <section>
        <h3>Synchronized rendering of web resources</h3>
        <p class="ednote">
          Describe gaps relating to synchronized rendering of web resources.
          Can we define a generic web API for scheduling page changes
          synchronized to playing media? Related: [[css-animations-1]],
          [[web-animations-1]], [[css-transitions-1]]. See also:
          <a href="https://github.com/bbc/VideoContext">https://github.com/bbc/VideoContext</a>.
          Should this be in scope for the TF?
        </p>
      </section>
      <section>
        <h3>Rendering of Web content embedded in media containers</h3>
        <p>
          There is no API for surfacing Web content embedded in ISO BMFF
          containers into the browser (e.g., the <code>HTMLCue</code> proposal
          discussed at <a href="https://www.w3.org/wiki/TPAC2015/HTMLcue">TPAC 2015</a>).
        </p>
        <p class="ednote">
          Add more detail on what's required. Some questions / considerations:
          <ul class="ednote">
            <li>Are the web resources intended to be handed to a Web application
            for rendering, or direct rendering by the UA?</li>
            <li>How do we guarantee that resources are delivered to the browser
            sufficiently ahead of time?</li>
            <li>How does same-origin policy affect such resources?</li>
          </ul>
        </p>
      </section>
    </section>
    <section>
      <h2>Recommendations</h2>
      <p class="ednote">
        Add recommendations here.
      </p>
    </section>
    <section>
      <h2>Acknowledgments</h2>
      <p>
        Thanks to Nigel Megitt and Jon Piesing for their contributions to this document.
      </p>
    </section>
  </body>
</html>
