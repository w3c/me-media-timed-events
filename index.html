<!DOCTYPE html>
<html>
  <head>
    <title>Media Timed Events</title>
    <meta charset="utf-8">
    <script src="https://www.w3.org/Tools/respec/respec-w3c-common" async class="remove"></script>
    <script class="remove">
      var respecConfig = {
        specStatus: "IG-NOTE",
        edDraftURI: "https://w3c.github.io/me-media-timed-events/",
        shortName: "media-timed-events",
        editors: [
          {
            name: "Chris Needham",
            mailto: "chris.needham@bbc.co.uk",
            company: "British Broadcasting Corporation",
            companyURL: "https://www.bbc.co.uk"
          },
          {
            name: "Giridhar Mandyam",
            mailto: "mandyam@qti.qualcomm.com",
            company: "Qualcomm",
            companyURL: "https://www.qualcomm.com"
          }
        ],
        wg: "Media & Entertainment Interest Group",
        wgURI: "https://www.w3.org/2011/webtv/",
        charterDisclosureURI: "https://www.w3.org/2017/03/webtv-charter.html",
        github: {
          repoURL: "https://github.com/w3c/me-media-timed-events/",
          branch: "master"
        },
        localBiblio: {
          "WEB-ISOBMFF": {
            title: "ISO/IEC JTC1/SC29/WG11 N16944 Working Draft on Carriage of Web Resources in ISOBMFF",
            href: "https://mpeg.chiariglione.org/standards/mpeg-4/timed-text-and-other-visual-overlays-iso-base-media-file-format/wd-carriage-web",
            // href: "https://mpeg.chiariglione.org/sites/default/files/files/standards/parts/docs/w16944.zip",
            authors: [
              "Thomas Stockhammer",
              "Cyril Concolato"
            ],
            publisher: "MPEG",
            date: "July 2017",
          },
          "DASH-EVENTING": {
            title: "DASH Eventing and HTML5",
            href: "https://www.w3.org/2011/webtv/wiki/images/a/a5/DASH_Eventing_and_HTML5.pdf",
            authors: [
              "Giridhar Mandyam"
            ],
            date: "February 2018"
          },
          "WEB-MEDIA-GUIDELINES": {
            title: "Web Media Application Developer Guidelines 2018",
            href: "https://w3c.github.io/webmediaguidelines/",
            authors: [
              "Joel Korpi",
              "Thasso Griebel",
              "Jeff Burtoft"
            ],
            publisher: "W3C",
            status: "CG-DRAFT",
            date: "26 April 2018"
          },
          "HBBTV": {
            title: "HbbTV 2.0.2 Specification",
            href: "https://www.hbbtv.org/wp-content/uploads/2018/02/HbbTV_v202_specification_2018_02_16.pdf",
            publisher: "HbbTV Association",
            date: "16 February 2018"
          },
          "HBBTV-TESTS": {
            title: "HbbTV Test Suite 2018-1",
            href: "https://www.hbbtv.org/wp-content/uploads/2018/03/HbbTV-testcases-2018-1.pdf",
            publisher: "HbbTV Association",
            date: "2018"
          },
          "DVB-DASH": {
            title: "DVB Document A168. Digital Video Broadcasting (DVB); MPEG-DASH Profile for Transport of ISO BMFF Based DVB Services over IP Based Networks",
            href: "https://www.dvb.org/resources/public/standards/a168_dvb_mpeg-dash_nov_2017.pdf",
            publisher: "DVB",
            date: "November 2017"
          },
          "BBC-SUBTITLES": {
            title: "Subtitle Guidelines",
            href: "http://bbc.github.io/subtitle-guidelines/",
            publisher: "BBC",
            date: "May 2018"
          },
          "3GPP-INTERACTIVITY-WID": {
            title: "SP-170796: New WID on 3GPP Service Interactivity",
            href: "http://www.3gpp.org/ftp/tsg_sa/TSG_SA/TSGS_77/Docs/SP-170796.zip",
            publisher: "3GPP",
            date: "September 2017"
          },
          "3GPP-INTERACTIVITY-TR": {
            title: "TR 26.953: Interactivity Support for 3GPP-Based Streaming and Download Services (Release 15)",
            href: "http://www.3gpp.org/ftp/Specs/archive/26_series/26.953/26953-f00.zip",
            publisher: "3GPP",
            date: "June 2018"
          },
          "WebVMT": {
            title: "WebVMT: The Web Video Map Tracks Format",
            href: "https://w3c.github.io/sdw/proposals/geotagging/webvmt/",
            authors: [
              "Rob Smith"
            ],
            publisher: "W3C",
            status: "ED",
            date: "11 October 2018"
          },
          "HLS-TIMED-METADATA": {
            title: "Timed Metadata for HTTP Live Streaming",
            href: "https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/HTTP_Live_Streaming_Metadata_Spec/Introduction/Introduction.html"
          }
        }
      };
    </script>
  </head>
  <body>
    <section id="abstract">
      <p>
        This document collects use cases and requirements for improved support
        for timed events related to audio or video media on the web, where
        synchronization to a playing audio or video media stream is needed,
        and makes recommendations for new or changed web APIs to realize these
        requirements. The goal is to extend the existing support in HTML for
        text track cue events to add support for dynamic content replacement
        cues and generic metadata events that drive synchronized interactive
        media experiences, and improve synchronization timing accuracy.
      </p>
    </section>
    <section id="sotd">
    </section>
    <section>
      <h2>Introduction</h2>
      <p>
        There is a need in the media industry for an API to support metadata
        events synchronized to audio or video media, specifically for both
        <a>out-of-band</a> event streams and <a>in-band</a> discrete events
        (for example, MPD and <code>emsg</code> events in MPEG-DASH).
        These <em>media timed events</em> can be used to support use cases
        such as dynamic content replacement, ad insertion, or presentation of
        supplemental content alongside the audio or video, or more generally,
        making changes to a web page, or executing application code triggered
        from JavaScript events, at specific points on the <a>media timeline</a>
        of an audio or video media stream.
      </p>
    </section>
    <section>
      <h2>Terminology</h2>
      <p>
        The following terms are used in this document:
      </p>
      <ul>
        <li>
          <dfn>in-band</dfn> &mdash; timed event information that is delivered
          within the audio or video media container or multiplexed with the
          media stream.
        </li>
        <li>
          <dfn>out-of-band</dfn> &mdash; timed event information that is
          delivered over some other mechanism external to the media container
          or media stream.
        </li>
      </ul>
      <p>
        The following terms are defined in [[HTML]]:
      </p>
      <ul>
        <li>
          <dfn><a href="https://html.spec.whatwg.org/multipage/media.html#media-timeline">media timeline</a></dfn>
        </li>
      </ul>
    </section>
    <section>
      <h2>Use cases</h2>
      <p>
        Media-timed events carry metadata that is related to points in time,
        or regions of time on the <a>media timeline</a>, which can be used to
        trigger retrieval and/or rendering of web resources synchronized with
        media playback. Such resources can be used to enhance user experience
        in the context of media that is being rendered. Some examples include
        display of social media feeds corresponding to a live broadcast such
        as a sporting event, banner advertisements for sponsored content,
        accessibility-related assets, such as large print rendering of
        captions, and display of track titles or images alongside an audio
        stream.
      </p>
      <p>
        The following sections describe a few use cases in more detail.
      </p>
      <section id="dynamic-content-insertion">
        <h3>Dynamic content insertion</h3>
        <p>
          A media content provider wants to allow insertion of content,
          such as personalised video, local news, or advertisements,
          into a video media stream that contains the main program content.
          To achieve this, media timed events used to describe the points
          on the <a>media timeline</a>, known as splice points, where switching
          playback to inserted content is possible.
        </p>
        <p>
          The Society for Cable and Televison Engineers (SCTE) specification
          "Digital Program Insertion Cueing for Cable" [[SCTE35]] defines a data
          cue format for describing such insertion points. Use of these cues in
          MPEG-DASH and HLS streams is described in [[SCTE35]], sections 12.1
          and 12.2.
        </p>
      </section>
      <section>
        <h3>Audio stream with titles and images</h3>
        <p>
          A media content provider wants to provide visual information alongside
          an audio stream, such as an image of the artist and title of the
          current playing track, to give users live information about the
          content they are listening to.
        </p>
        <p>
          Examples include HLS timed metadata [[HLS-TIMED-METADATA]], which uses
          <a>in-band</a> ID3 metadata to carry the image content, and RadioVIS
          in DVB ([[DVB-DASH]], section 9.1.7), which defines <a>in-band</a>
          event messages that contain image URLs and text messages to be
          displayed, with information about when the content should be displayed
          in relation to the <a>media timeline</a>.
        </p>
      </section>
      <section>
        <h3>MPEG-DASH manifest expiry notifications</h3>
        <p>
          Section 5.10.4 of [[MPEGDASH]] describes an MPEG-DASH specific event
          that is used to notify a DASH player web application that it should
          refresh its copy of the manifest (MPD) document.
          An <a>in-band</a> <code>emsg</code> event is used an alternative to setting
          a cache duration in the response to the HTTP request for the manifest,
          so the client can refresh the MPD when it actually changes, so reducing
          the load on HTTP servers caused by frequent server requests.
        </p>
        <p>
          Reference: M&amp;E IG call 1 Feb 2018:
          <a href="https://www.w3.org/2018/02/01-me-minutes.html">Minutes</a>,
          [[DASH-EVENTING]].
        </p>
        <p class="ednote">
          See also <a href="https://github.com/w3c/webmediaguidelines/issues/64">this issue</a>
          against the [[WEB-MEDIA-GUIDELINES]]. TODO: Add detail here.
        </p>
      </section>
      <section>
        <h3>Subtitle and caption rendering synchronization</h3>
        <p>
          A subtitle or caption author wants ensure that subtitle changes are
          aligned as closely as possible to shot changes in the video.
          The BBC Subtitle Guidelines [[BBC-SUBTITLES]] describes authoring
          best practices. In particular, in section 6.1 authors are advised
          "it is likely to be less tiring for the viewer if shot changes
          and subtitle changes occur at the same time. Many subtitles therefore
          start on the first frame of the shot and end on the last frame."
        </p>
      </section>
      <section>
        <h3>Synchronized map animations</h3>
        <p>
          A user records footage with metadata, including geolocation, on a
          mobile video device, e.g., drone or dashcam, to share on the web
          alongside a map, e.g., OpenStreetMap.
        </p>
        <p>
          [[WebVMT]] is an open format for metadata cues, synchronized with a
          timed media file, that can be used to drive an online map rendered in
          a separate HTML element alongside the media element on the web page.
          The media playhead position controls presentation and animation of the
          map, e.g., pan and zoom, and allows annotations to be added and
          removed, e.g., markers, at specified times during media playback.
          Control can also be overridden by the user with the usual interactive
          features of the map at any time, e.g., zoom. Concrete examples are
          provided by the <a href="http://webvmt.org/demos">tech demos</a> at
          the WebVMT website.
        </p>
        <p>
          Reference: M&amp;E IG TF call 17 Sept 2018:
          <a href="https://www.w3.org/2018/09/17-me-minutes.html">Minutes</a>.
        </p>
      </section>
      <section>
        <h3>Media analysis visualization</h3>
        <p>
          A video image analysis system processes a media stream to detect and
          recognize objects shown in the video. This system generates metadata
          describing the objects, including timestamps that describe the when
          the objects are visible, together with position information (e.g.,
          bounding boxes). A web application then uses this timed metadata to
          overlay labels and annotations on the video using HTML and CSS.
        </p>
      </section>
      <section>
        <h3>Presentation of auxiliary content in live media</h3>
        <p>
          During a live media presentation, dynamic and unpredictable events
          may occur which cause temporary suspension of the media presentation.
          During that suspension interval, auxiliary content such as the presentation
          of UI controls and media files, may be unavailable. Depending on the
          specific user engagement (or not) with the UI controls and the time
          at which any such engagement occurs, specific web resources may be
          rendered at defined times in a synchronized manner. For example,
          a multimedia A/V clip along with subtitles corresponding to an
          advertisement, and which were previously downloaded and cached
          by the UA, are played out.
        </p>
      </section>
    </section>
    <section>
      <h2>Related industry specifications</h2>
      <p>
        This section describes existing media industry specifications and
        standards that specify carriage of media timed events, or otherwise
        provide requirements for web APIs related to the triggering of media
        timed events.
      </p>
      <section>
        <h3>MPEG Common Media Application Format (CMAF)</h3>
        <p>
          MPEG Common Media Application Format (CMAF) [[MPEGCMAF]] is a media
          container format optimized for large scale delivery of a single
          encrypted, adaptable multimedia presentation to a wide range of
          devices and adaptive streaming methods, including HTTP Live Streaming
          [[RFC8216]] and MPEG-DASH [[MPEGDASH]]. It is based on the ISO BMFF
          [[ISOBMFF]] and supports the AVC, AAC, HEVC codecs, Common Encryption
          (CENC), and subtitles using IMSC1 and WebVTT. The goal is to reduce
          media storage and delivery costs by using a single common media format
          across different client devices.
        </p>
        <p>
          CMAF media may contain <a>in-band</a> events in the form of
          Event Message (<code>emsg</code>) boxes in ISO BMFF files.
          <code>emsg</code> is specified in [[MPEGDASH]], section 5.10.3.3,
          and described in more detail in the following section of this
          document.
        </p>
      </section>
      <section>
        <h3>MPEG-DASH</h3>
        <p>
          MPEG-DASH is an adaptive bitrate streaming technique in which the
          audio and video media is partitioned into segments. The Media
          Presentation Description (MPD) is an XML document that contains
          metadata required by a DASH client to access the media segments and to
          provide the streaming service to the user. The media segments can use
          any codec, typically within a fragmented MP4 (ISO BMFF) container or
          MPEG-2 transport stream.
        </p>
        <p>
          In MPEG-DASH, media timed events may be delivered either
          <a>in-band</a> or <a>out-of-band</a>:
        </p>
        <ul>
          <li>
            <a>In-band</a> events are <code>emsg</code> boxes in ISO BMFF files.
            The presence of <code>emsg</code> events in the media container for
            given event schemes is signalled in the MPD document using an
            <code>EventStream</code> XML element ([[MPEGDASH]], section 5.10.2).
          </li>
          <li>
            <a>Out-of-band</a> events are represented by <code>Event</code>
            XML elements </code>contained within an <code>EventStream</code>
            element in the MPD.
          </li>
        </ul>
        <p>
          An <code>emsg</code> event contains the following information,
          as specified in [[MPEGDASH]], section 5.10.3.3:
        </p>
        <ul>
          <li><code>scheme_id_uri</code> &mdash; A URI that identifies
          the message scheme</li>
          <li><code>value</code> &mdash; The event value (string)</li>
          <li><code>timescale</code> &mdash; Timescale units, in ticks
          per second</li>
          <li><code>presentation_time_delta</code> &mdash; Presentation
          time delta (with respect to the media segment),
          in <code>timescale</code> units</li>
          <li><code>event_duration</code> &mdash; Event duration,
          in <code>timescale</code> units</li>
          <li><code>id</code> &mdash; Event message identifier</li>
          <li><code>message_data</code> &mdash; Message body (may be empty)</li>
        </ul>
      </section>
      <section>
        <h3>HbbTV</h3>
        <p>
          HbbTV is an interactive TV application standard that supports both
          broadcast (DVB) media delivery, and internet streaming using
          MPEG-DASH. The HbbTV application environment is based on HTML and
          JavaScript. MPEG-DASH streaming is implemented nativey by the user
          agent, rather than through a JavaScript web application using Media
          Source Extensions.
        </p>
        <p>
          HbbTV includes support for <code>emsg</code> events ([[DVB-DASH]],
          section 9.1) and requires this be mapped to HTML5 <code>DataCue</code>
          ([[HBBTV]], section 9.3.2). The revision of HTML5 referenced
          by [[HBBTV]] is [[html51-20151008]]. This feature is included in user
          agents shipping in connected TVs across Europe from 2017.
        </p>
        <p>
          The HbbTV device test suite includes test pages and streams that
          cover <code>emsg</code> support ([[HBBTV-TESTS]]). HbbTV has a
          <a href="https://github.com/HbbTV-Association/ReferenceApplication">reference application</a>
          and content for DASH+DRM which includes <code>emsg</code> support.
        </p>
      </section>
      <section>
        <h3>DASH Industry Forum APIs for Interactivity</h3>
        <p>
          The DASH-IF InterOp Working Group has an ongoing work item,
          <em>DAInty</em>, "DASH APIs for Interactivity", which aims to
          specify a set of APIs between the DASH client/player and interactivity-capable
          applications, for both web and native applications. The origin of this
          work is a related 3GPP work item on Service Interactivity
          ([[3GPP-INTERACTIVITY-WID]], [[3GPP-INTERACTIVITY-TR]]).
          The objective is to provide service enablers for user engagement with
          auxiliary content and UIs on mobile device during live or time-shifted
          viewing of streaming content delivered over 3GPP broadcast or unicast
          bearers, and the measurement and reporting of such interactive consumption.
        </p>
        <p>
          Two APIs are being developed that are relevant to the scope of the present
          document:
        </p>
        <ul>
          <li>
            Application subscription/DASH client dispatch of DASH event stream
            messages containing interactivity information. Events can be delivered
            <a>in-band</a> (<code>emsg</code>) and/or as MPD events.
          </li>
          <li>
            Application subscription/DASH client dispatch of ISO BMFF Timed
            Metadata tracks providing similar functionality to DASH event streams.
          </li>
        </ul>
        <p>
          Two modes for dispatching events are defined. In Mode 1, events are dispatched
          at the time the event arrives, and in Mode 2, events are dispatched at the
          given time on the <a>media timeline</a>. The "arrival" of events from the DASH client
          perspective may be either static or pre-provisioned, in the case MPD Events,
          or dynamic in the case of inband events carried in the <code>emsg</code>.
          The application can register with the DASH client which Mode to use.
        </p>
        <p>
          Reference: M&amp;E IG, Media Timed Events Task Force call 20 Aug 2018:
          <a href="https://www.w3.org/2018/08/20-me-minutes.html">Minutes</a>.
        </p>
      </section>
      <section>
        <h3>SCTE-35</h3>
        <p>
          The Society for Cable and Televison Engineers (SCTE) has produced the
          SCTE-35 specification "Digital Program Insertion Cueing for Cable"
          [[SCTE35]], which defines a data cue format for describing
          insertion points, to support the
          <a href="#dynamic-content-insertion">dynamic content insertion</a> use
          case.
        </p>
        <p>
          [[SCTE214-1]] section 6.7 describes the carriage of SCTE-35 events
          in a MPEG-DASH MPD document, as <a>out-of-band</a> events.
          [[SCTE214-2]] section 9 and [[SCTE214-3]] section 7.3 describe
          the carriage of SCTE-35 events as <a>in-band</a> events in MPEG-DASH
          using MPEG2-TS and ISO BMFF respectively, using <code>emsg</code>.
        </p>
          [[SCTE35]] section 9.1 describes the requirements for content
          splicing: "In order to give advance warning of the impending splice
          (a pre-roll function), the splice_insert() command could be sent
          multiple times before the splice point. For example, the
          splice_insert() command could be sent at 8, 5, 4 and 2 seconds prior
          to the packet containing the related splice point. In order to meet
          other splicing deadlines in the system, any message received with less
          than 4 seconds of advance notice may not create the desired result."
        </p>
        <p>
          This places an implicit requirement on the user agent in handling
          media-timed events related to insertion cues. The content originator
          may provide the cue in advance with as little as 2 seconds of the
          insertion time. Therefore the propagation of the event data associated
          with the insertion cue to the application by the user agent should be
          considerably less than 2 seconds.
        </p>
      </section>
      <section>
        <h3>MPEG Working Draft on Carriage of Web Resources in ISO BMFF</h3>
        <p>
          The MPEG Working Draft on Carriage of Web Resources in ISO BMFF
          [[WEB-ISOBMFF]] is a draft document that specifies the use of the
          ISO BMFF container format for the storage and delivery of web
          content. The goal is to allow web resources (HTML, JavaScript, etc.)
          to be parsed from the storage and processed by a user agent at
          specific presentation times on the <a>media timeline</a>, and so be
          synchronized with other tracks within the container, such as audio,
          video, and subtitles.
        </p>
        <p>
          The Media &amp; Entertainment Interest Group is actively tracking
          this work is open to discussing specific requirements for media
          timed events as development progresses.
        </p>
      </section>
      <section>
        <h3>WebVTT</h3>
        <p>
          [[WEBVTT]] is a W3C specification that provides a format for web video
          text tracks. A <code>VTTCue</code> is a text track cue, and may have
          attributes that affect rendering of the cue text on a web page.
          WebVTT metadata cues are text that is aligned to the
          <a>media timeline</a>. Web applications can use <code>VTTCue</code>
          to schedule <a>out-of-band</a> metadata events by serializing the
          event data to a string format (JSON, for example) when creating the
          cue, and deserializing the data when the cue is triggered.
        </p>
        <p>
          Web applications can also use <code>VTTCue</code> to trigger
          rendering of <a>out-of-band</a> delivered timed text cues, such as
          TTML or IMSC format captions.
        </p>
      </section>
    </section>
    <section>
      <h2>Gap analysis</h2>
      <p>
        This section describes gaps in existing existing web platform
        capabilities needed to support the use cases and requirements described
        in this document. Where applicable, this section also describes how
        existing web platform features can be used as workarounds, and any
        associated limitations.
      </p>
      <section>
        <h4>MPEG-DASH and ISO BMFF emsg events</h4>
        <p>
          The <code>DataCue</code> API has been previously discussed as a means to
          deliver <a>in-band</a> event data to web applications, but this is not implemented
          in all of the main browser engines. It is <a href="https://www.w3.org/TR/2018/WD-html53-20181018/semantics-embedded-content.html#text-tracks-exposing-inband-metadata">included</a>
          in the 18 October 2018 HTML 5.3 draft [[HTML53-20181018]], but is
          <a href="https://html.spec.whatwg.org/multipage/media.html#timed-text-tracks">not included</a>
          in [[HTML]]. See discussion <a href="https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/U06zrT2N-Xk">here</a>
          and notes on implementation status <a href="https://lists.w3.org/Archives/Public/public-html/2016Apr/0005.html">here</a>.
        </p>
        <p>
          WebKit <a href="https://discourse.wicg.io/t/media-timed-events-api-for-mpeg-dash-mpd-and-emsg-events/3096/2">supports</a>
          a <code>DataCue</code> interface that extends HTML5 <code>DataCue</code>
          with two attributes to support non-text metadata, <code>type</code> and
          <code>value</code>.
        </p>
        <pre class="example">
          interface DataCue : TextTrackCue {
            attribute ArrayBuffer data; // Always empty

            // Proposed extensions.
            attribute any value;
            readonly attribute DOMString type;
          };
        </pre>
        <p>
          <code>type</code> is a string identifying the type of metadata:
        </p>
        <table class="simple">
          <thead>
            <tr>
              <th colspan="2">WebKit <code>DataCue</code> metadata types</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>"com.apple.quicktime.udta"</code></td>
              <td>QuickTime User Data</td>
            </tr>
            <tr>
              <td><code>"com.apple.quicktime.mdta"</code></td>
              <td>QuickTime Metadata</td>
            </tr>
            <tr>
              <td><code>"com.apple.itunes"</code></td>
              <td>iTunes metadata</td>
            </tr>
            <tr>
              <td><code>"org.mp4ra"</code></td>
              <td>MPEG-4 metadata</td>
            </tr>
            <tr>
              <td><code>"org.id3"</code></td>
              <td>ID3 metadata</td>
          </tr>
          </tbody>
        </table>
        <p>
          and <code>value</code> is an object with the metadata item key, data, and optionally a locale:
        </p>
        <pre class="example">
          value = {
            key: String
            data: String | Number | Array | ArrayBuffer | Object
            locale: String
          }
        </pre>
        <p>
          Neither [[MSE-BYTE-STREAM-FORMAT-ISOBMFF]] nor [[INBANDTRACKS]] describe
          handling of <code>emsg</code> boxes.
        </p>
        <p>
          On resource constrained devices such as smart TVs and streaming sticks,
          parsing media segments to extract event information leads to a significant
          performance penalty, which can have an impact on UI rendering updates if
          this is done on the UI thread. There can also be an impact on the battery
          life of mobile devices. Given that the media segments will be parsed anyway
          by the user agent, parsing in JavaScript is an expensive overhead that
          could be avoided.
        </p>
        <p>
          [[HBBTV]] section 9.3.2 describes a mapping between the <code>emsg</code>
          fields described <a href="#mpeg-dash">above</a>
          and the <a href="https://html.spec.whatwg.org/multipage/media.html#texttrack"><code>TextTrack</code></a>
          and <a href="https://www.w3.org/TR/2018/WD-html53-20180426/semantics-embedded-content.html#datacue"><code>DataCue</code></a>
          APIs. A <code>TextTrack</code> instance is created for each event
          stream signalled in the MPD document (as identified by the
          <code>schemeIdUri</code> and <code>value</code>), and the
          <a href="https://html.spec.whatwg.org/multipage/media.html#dom-texttrack-inbandmetadatatrackdispatchtype"><code>inBandMetadataTrackDispatchType</code></a>
          <code>TextTrack</code> attribute contains the <code>scheme_id_uri</code>
          and <code>value</code> values. Because HbbTV devices include a native
          DASH client, parsing of the MPD document and creation of the
          <code>TextTrack</code>s is done by the user agent, rather than by
          application JavaScript code.
        </p>
        <p class="ednote">
          To support DASH clients implemented in web applications, there is
          therefore either a need for an API that allows applications to tell
          the UA which schemes it wants to receive, or the UA should simply
          expose all event streams to applications. Which of these is preferred?
        </p>
      </section>
      <section>
        <h3>Synchronization of text track cue rendering</h3>
        <p>
          Subtitles for video are typically authored against video at
          a nominal frame rate, e.g., 25 frames per second, which corresponds to
          40 milliseconds per frame. The actual video frame rate may be adjusted
          dynamically according to the video encoding, but the subtitle timing
          must remain the same ([[EBU-TT-D]], Annex E).
        </p>
        <p>
          This places a requirement on user agents for timely delivery of
          <code>TextTrackCue</code> and <code>VTTCue</code> events, so that
          application code can respond and render the cues. For subtitle
          rendering to be possible with frame accuracy, we recommend that cue
          events are fired within 20 milliseconds of their position on the
          <a>media timeline</a>.
        </p>
        <p>
          The <a href="https://html.spec.whatwg.org/multipage/media.html#time-marches-on"><em>time
          marches on</em></a> steps in [[HTML]] control the firing of cue
          events during media playback. <em>Time marches on</em> requires a
          <code>timeupdate</code> event to be fired at the
          <code>HTMLMediaElement</code> between 15 and 250 milliseconds since
          the last such event, and this requirement therefore specifies the rate
          at which <em>time marches on</em> is executed during playback. In
          practice it <a href="https://www.w3.org/2018/12/17-me-minutes.html#item06">has
          been found</a> that the timing varies between browser implementations.
        </p>
        <p>
          There are two methods a web application can use to handle text track
          cues:
        </p>
        <p>
          <ul>
            <li>
              Add an <code>oncuechange</code> handler function to the
              <code>TextTrack</code> and inspect the track's
              <code>activeCues</code> list. Because <code>activeCues</code>
              contains the list of cues that are active at the time that
              <em>time marches on</em></a> is run, it is possible for cues to be
              missed by a web application using this method, where cues appear
              on the <a>media timeline</a> between successive executions of
              <em>time marches on</em> during media playback. This may occur
              if the cues have short duration, or by a long-running event
              handler function.
            </li>
            <li>
              Add <code>onenter</code> and <code>onexit</code> handler functions
              to each cue. The <em>time marches on</em> steps guarantee that
              <code>enter</code> and <code>exit</code> events will be fired for
              all cues, including those that appear on the <a>media timeline</a>
              between successive executions of <em>time marches on</em>
              during media playback. This method is only possible for cues
              created by the web application, i.e., <code>VTTCue</code> objects,
              and not cue objects created by the user agent.
            </li>
          </ul>
        </p>
        <p>
          An issue with handling of text track and data cue events in HbbTV
          <a href="https://lists.w3.org/Archives/Public/public-inbandtracks/2013Dec/0004.html">was
          reported</a> in 2013. HbbTV requires the user agent to implement an
          MPEG-DASH client, and so applications must use the first of the above
          methods for cue handling, which means that applications can miss cues
          as described above.
        </p>
      </section>
      <section>
        <h3>Synchronized rendering of web resources</h3>
        <p class="ednote">
          Describe gaps relating to synchronized rendering of web resources.
          Can we define a generic web API for scheduling page changes
          synchronized to playing media? Related: [[css-animations-1]],
          [[web-animations-1]], [[css-transitions-1]]. See also:
          <a href="https://github.com/bbc/VideoContext">https://github.com/bbc/VideoContext</a>.
          Should this be in scope for the TF?
        </p>
      </section>
      <section>
        <h3>Rendering of web content embedded in media containers</h3>
        <p>
          There is no API for surfacing web content embedded in ISO BMFF
          containers into the browser (e.g., the <code>HTMLCue</code> proposal
          discussed at <a href="https://www.w3.org/wiki/TPAC2015/HTMLcue">TPAC 2015</a>).
        </p>
        <p class="ednote">
          Add more detail on what's required. Some questions / considerations:
        </p>
        <ul class="ednote">
          <li>Are the web resources intended to be handed to a web application
          for rendering, or direct rendering by the UA?</li>
          <li>How do we guarantee that resources are delivered to the browser
          sufficiently ahead of time?</li>
          <li>How does same-origin policy affect such resources?</li>
        </ul>
      </section>
    </section>
    <section>
      <h2>Recommendations</h2>
      <p>
        This section describes recommendations from the Media &amp; Entertainment
        Interest Group for the development of a generic media timed event API,
        and associated synchronization considerations.
      </p>
      <section>
        <h3>Subscribing to event streams</h3>
        <p>
          The API should allow web applications to subscribe to receive specific
          event types. For example, to support DASH <code>emsg</code> and MPD events,
          the API should allow subscription by <code>id</code> and (optional) <code>value</code>.
          This is to make receiving events opt-in from the application point of view.
          The user agent should deliver only those events to a web application
          for which the application has subscribed. The API should also allow web
          applications to unsubscribe from specific event streams by event type.
        </p>
      </section>
      <section>
        <h3>Out-of-band events</h3>
        <p>
          To be able to handle out of band events, the API must allow web
          applications to create events to be added to the <a>media timeline</a>,
          to be triggered by the user agent. The API should allow the
          web application to provide all necessary parameters to define
          the event, including start and end times, event type, and data
          payload. The payload should be any data type (e.g., the set of
          types supported by the WebKit <code>DataCue</code>). For DASH MPD
          events, the event type is defined by the <code>id</code> and
          (optional) <code>value</code> fields.
        </p>
      </section>
      <section>
        <h3>Event triggering</h3>
        <p>
          For those events that the application has subscribed to receive,
          the API should:
        </p>
        <ul>
          <li>
            Generate a JavaScript event when an <a>in-band</a> media timed event
            is parsed from the media container or media stream (DAInty Mode 1).
          </li>
          <li>
            Generate JavaScript events when the current media playback
            position reaches the start time and the end time of a media timed
            event during playback (DAInty Mode 2). This applies equally to
            <a>in-band</a> events that the user agent has extracted from the
            media container, and <a>out-of-band</a> events added by the web
            application.
          </li>
        </ul>
        <p>
          The API must provide guarantees that no events can be missed during
          linear playback of the media.
        </p>
      </section>
      <section>
        <h3>In-band event processing</h3>
        <p>
          We recommend updating [[INBANDTRACKS]] to describe handling of
          <a>in-band</a> media timed events supported on the web platform,
          following a registry approach with one specification per media format
          that describes the event details for that format.
        </p>
      </section>
      <section>
        <h3>MPEG-DASH events</h3>
        <p>
          We recommend that browser engines support MPEG-DASH <code>emsg</code>
          <a>in-band</a> events and MPD <a>out-of-band</a> events, as part of
          their support for the MPEG Common Media Application Format (CMAF)
          [[MPEGCMAF]].
        </p>
      </section>
      <section>
        <h3>Synchronization</h3>
        <p>
          In order to acheive greater synchronization accuracy between media
          playback and web content rendered by an application, the <em>time
          marches on</em> steps in [[HTML]] should be modified to allow delivery
          of media timed event start time and end time notifications within 20
          milliseconds of their positions on the <a>media timeline</a>.
        </p>
      </section>
    </section>
    <section>
      <h2>Acknowledgments</h2>
      <p>
        Thanks to Charles Lo, Nigel Megitt, Jon Piesing, and Rob Smith for their
        contributions to this document.
      </p>
    </section>
  </body>
</html>
