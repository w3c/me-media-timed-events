<!DOCTYPE html>
<html>
  <head>
    <title>Media Timed Events</title>
    <meta charset="utf-8">
    <script src="https://www.w3.org/Tools/respec/respec-w3c-common" async class="remove"></script>
    <script class="remove">
      var respecConfig = {
        specStatus: "ED",
        edDraftURI: "https://w3c.github.io/me-media-timed-events/",
        shortName: "media-timed-events",
        editors: [
          {
            name: "Chris Needham",
            mailto: "chris.needham@bbc.co.uk",
            company: "British Broadcasting Corporation",
            companyURL: "https://www.bbc.co.uk"
          },
          {
            name: "Giridhar Mandyam",
            mailto: "mandyam@qti.qualcomm.com",
            company: "Qualcomm",
            companyURL: "https://www.qualcomm.com"
          }
        ],
        wg: "Media & Entertainment Interest Group",
        wgURI: "https://www.w3.org/2011/webtv/",
        charterDisclosureURI: "https://www.w3.org/2017/03/webtv-charter.html",
        wgPublicList: "public-web-and-tv",
        localBiblio: {
          "WEB-ISOBMFF": {
            title: "ISO/IEC JTC1/SC29/WG11 N16944 Working Draft on Carriage of Web Resources in ISOBMFF",
            href: "https://mpeg.chiariglione.org/standards/mpeg-4/timed-text-and-other-visual-overlays-iso-base-media-file-format/wd-carriage-web",
            // href: "https://mpeg.chiariglione.org/sites/default/files/files/standards/parts/docs/w16944.zip",
            authors: [
              "Thomas Stockhammer",
              "Cyril Concolato"
            ],
            publisher: "MPEG",
            date: "July 2017",
          },
          "DASH-EVENTING": {
            title: "DASH Eventing and HTML5",
            href: "https://www.w3.org/2011/webtv/wiki/images/a/a5/DASH_Eventing_and_HTML5.pdf",
            authors: [
              "Giridhar Mandyam"
            ],
            date: "February 2018"
          },
          "WEBVTT": {
            title: "WebVTT: The Web Video Text Tracks Format",
            href: "https://www.w3.org/TR/webvtt1/",
            authors: [
              "Simon Pieters",
              "Silvia Pfeiffer",
              "Phillip JÃ¤genstedt",
              "Ian Hickson"
            ],
            publisher: "W3C",
            status: "CR",
            date: "10 May 2018"
          },
          "WEB-MEDIA-GUIDELINES": {
            title: "Web Media Application Developer Guidelines 2018",
            href: "https://w3c.github.io/webmediaguidelines/",
            authors: [
              "Joel Korpi",
              "Thasso Griebel",
              "Jeff Burtoft"
            ],
            publisher: "W3C",
            status: "CG-DRAFT",
            date: "26 April 2018"
          },
          "HBBTV": {
            title: "HbbTV 2.0.2 Specification",
            href: "https://www.hbbtv.org/wp-content/uploads/2018/02/HbbTV_v202_specification_2018_02_16.pdf",
            publisher: "HbbTV Association",
            date: "16 February 2018"
          },
          "HBBTV-TESTS": {
            title: "HbbTV Test Suite 2018-1",
            href: "https://www.hbbtv.org/wp-content/uploads/2018/03/HbbTV-testcases-2018-1.pdf",
            publisher: "HbbTV Association",
            date: "2018"
          },
          "DVB-DASH": {
            title: "DVB Document A168. Digital Video Broadcasting (DVB); MPEG-DASH Profile for Transport of ISO BMFF Based DVB Services over IP Based Networks",
            href: "https://www.dvb.org/resources/public/standards/a168_dvb_mpeg-dash_nov_2017.pdf",
            publisher: "DVB",
            date: "November 2017"
          },
          "BBC-SUBTITLES": {
            title: "Subtitle Guidelines",
            href: "http://bbc.github.io/subtitle-guidelines/",
            publisher: "BBC",
            date: "May 2018"
          },
          "3GPP-INTERACTIVITY-WID": {
            title: "SP-170796: New WID on 3GPP Service Interactivity",
            href: "http://www.3gpp.org/ftp/tsg_sa/TSG_SA/TSGS_77/Docs/SP-170796.zip",
            publisher: "3GPP",
            date: "September 2017"
          },
          "3GPP-INTERACTIVITY-TR": {
            title: "TR 26.953: Interactivity Support for 3GPP-Based Streaming and Download Services (Release 15)",
            href: "http://www.3gpp.org/ftp/Specs/archive/26_series/26.953/26953-f00.zip",
            publisher: "3GPP",
            date: "June 2018"
          },
          "WebVMT": {
            title: "WebVMT: The Web Video Map Tracks Format",
            href: "https://w3c.github.io/sdw/proposals/geotagging/webvmt/",
            authors: [
              "Rob Smith"
            ],
            publisher: "W3C",
            status: "ED",
            date: "11 October 2018"
          }
        }
      };
    </script>
  </head>
  <body>
    <section id="abstract">
      <p>
        This document collects use cases and requirements for improved support
        for timed events related to audio or video media on the Web, such as
        subtitles, captions, or other web content, where synchronization to a
        playing audio or video media stream is needed, and makes recommendations
        for new or changed Web APIs to realize these requirements.
      </p>
    </section>
    <section id="sotd">
    </section>
    <section>
      <h2>Introduction</h2>
      <p>
        Media timed events describes a generic capability for making
        changes to a Web page, or executing application code triggered from
        JavaScript events, at specific points on the media timeline of an
        audio or video media stream.
      </p>
    </section>
    <section>
      <h2>Terminology</h2>
      <p>
        The following terms are used in this document:
        <ul>
          <li>
            <dfn>in-band</dfn> &mdash; timed event information that is delivered
            within the audio or video media container or multiplexed with the
            media stream.
          </li>
          <li>
            <dfn>out-of-band</dfn> &mdash; timed event information that is
            delivered over some other mechanism external to the media container
            or media stream.
          </li>
        </ul>
      </p>
      <p>
        The following terms are defined in [[HTML52]]:
        <ul>
          <li>
            <dfn><a href="https://www.w3.org/TR/html52/semantics-embedded-content.html#media-timeline">media timeline</a></dfn>
          </li>
        </ul>
      </p>
    </section>
    <section>
      <h2>Use cases</h2>
      <p>
        Media-timed events carry metadata that is related to points in time,
        or regions of time on the media timeline, which can be used to trigger
        retrieval and/or rendering of web resources synchronized with media
        playback. Such resources can be used to enhance user experience in
        the context of media that is being rendered. Some examples include
        display of social media feeds corresponding to a live broadcast such
        as a sporting event, banner advertisements for sponsored content,
        accessibility-related assets, such as large print rendering of
        captions, and display of track titles or images alongside an audio
        stream.
      </p>
      <p>
        The following sections describe a few use cases in more detail.
      </p>
      <section>
        <h3>MPEG DASH manifest expiry notifications</h3>
        <p>
          Section 5.10.4 of [[MPEGDASH]] describes a DASH specific event that
          is used to notify a DASH player Web application that it should refresh
          its copy of the manifest (MPD) document.
          An in-band <code>emsg</code> event is used an alternative to setting
          a cache duration in the response to the HTTP request for the manifest,
          so the client can refresh the MPD when it actually changes, so reducing
          the load on HTTP servers caused by frequent server requests.
        </p>
        <p>
          Reference: M&amp;E IG call 1 Feb 2018:
          <a href="https://www.w3.org/2018/02/01-me-minutes.html">Minutes</a>,
          [[DASH-EVENTING]].
        </p>
        <p class="ednote">
          See also <a href="https://github.com/w3c/webmediaguidelines/issues/64">this issue</a>
          against the [[WEB-MEDIA-GUIDELINES]]. TODO: Add detail here.
        </p>
      </section>
      <section>
        <h3>Synchronized map animations</h3>
        <p>
          [[WebVMT]] is a format for metadata cues, synchronised with
          a timed media file, that can drive an online map, e.g., OpenStreetMap,
          rendered in a separate HTML element alongside the media element
          on the web page. The media playhead position controls presentation
          and animation of the map, e.g., pan and zoom, and allows annotations
          to be added and removed, e.g., markers, at specified times during
          media playback. Control can also be overridden by the user with the
          usual interactive features of the map at any time, e.g., zoom.
          Concrete examples are provided by the
          <a href="http://webvmt.org/demos">tech demos</a> at the WebVMT website.
        </p>
        <p>
          Reference: M&amp;E IG TF call 17 Sept 2018:
          <a href="https://www.w3.org/2018/09/17-me-minutes.html">Minutes</a>.
        </p>
        <p class="ednote">
          Add use case descriptions for synchronised rendering here. Note that
          this could be rendering of any web resource, not necessarily those
          embedded in media containers. Describe a few motivating application
          scenarios.
        </p>
      </section>
      <section>
        <h3>Presentation of auxiliary content in live media</h3>
        <p>
          During a live media presentation, dynamic and unpredictable events
          may occur which cause temporary suspension of the media presentation.
          During that suspension interval, auxiliary content such as the presentation
          of UI controls and media files, may be unavailable. Depending on the
          specific user engagement (or not) with the UI controls and the time
          at which any such engagement occurs, specific web resources may be
          rendered at defined times in a synchronized manner. For example,
          a multimedia A/V clip along with subtitles corresponding to an
          advertisement, and which were previously downloaded and cached
          by the UA, are played out.
        </p>
      </section>
    </section>
    <section>
      <h2>Related industry standards</h2>
      <section>
        <h3>MPEG-DASH</h3>
        <p>
          In MPEG DASH, events may be conveyed either as in-band events, e.g.,
          as <code>emsg</code> boxes in ISO BMFF files, or out-of-band, via an
          EventStream fragment in the MPD (Media Presentation Description)
          document (i.e., by an instance of the <code>EventStream</code> child
          of the <code>MPD.Period</code> element).
          In addition, the MPD document may advertise the presence of
          <code>emsg</code> events in the ISO BMFF content for given schemas.
        </p>
        <p>
          An <code>emsg</code> event contains the following information,
          as specified in [[MPEGDASH]], section 5.10.3.3:
          <ul>
            <li><code>scheme_id_uri</code> &mdash; A URI that identifies
            the message scheme</li>
            <li><code>value</code> &mdash; The event value (string)</li>
            <li><code>timescale</code> &mdash; Timescale units, in ticks
            per second</li>
            <li><code>presentation_time_delta</code> &mdash; Presentation
            time delta (with respect to the media segment),
            in <code>timescale</code> units</li>
            <li><code>event_duration</code> &mdash; Event duration,
            in <code>timescale</code> units</li>
            <li><code>id</code> &mdash; Event message identifier</li>
            <li><code>message_data</code> &mdash; Message body (may be empty)</li>
          </ul>
        </p>
        <p>
          The presence of <code>emsg</code> events in the media stream is signalled
          in the DASH manifest document (MPD), using an <code>EventStream</code> XML
          element ([[MPEGDASH]], section 5.10.2).
        </p>
      </section>
      <section>
        <h3>HbbTV</h3>
        <p>
          HbbTV includes support for <code>emsg</code> events ([[DVB-DASH]],
          section 9.1) and requires this be mapped to <code>DataCue</code>
          ([[HBBTV]], section 9.3.2). The HbbTV device test suite includes
          test pages and streams for this ([[HBBTV-TESTS]]). This feature is
          included in user agents shipping in connected TVs across Europe from
          2017. HbbTV has a <a href="https://github.com/HbbTV-Association/ReferenceApplication">reference app</a>
          and content for DASH+DRM which includes <code>emsg</code> support.
          As well as HbbTV devices, this reference app and content have been
          tested on Microsoft Edge with MSE and EME although <code>emsg</code>
          support does not work there.
        </p>
      </section>
      <section>
        <h3>DASH Industry Forum APIs for Interactivity</h3>
        <p>
          The DASH-IF InterOp Working Group has an ongoing work item,
          <emph>DAInty</emph>, "DASH APIs for Interactivity", which aims to
          specify a set of APIs between the DASH client/player and interactivity-capable
          applications, for both Web and native applications. The origin of this
          work is a related 3GPP work item on Service Interactivity
          ([[3GPP-INTERACTIVITY-WID]], [[3GPP-INTERACTIVITY-TR]]).
          The objective is to provide service enablers for user engagement with
          auxiliary content and UIs on mobile device during live or time-shifted
          viewing of streaming content delivered over 3GPP broadcast or unicast
          bearers, and the measurement and reporting of such interactive consumption.
        </p>
        <p>
          Two APIs are being developed that are relevant to the scope of the present
          document:
          <ul>
            <li>
              Application subscription/DASH client dispatch of DASH event stream
              messages containing interactivity information. Events can be delivered
              in-band (<code>emsg</code>) and/or as MPD events.
            </li>
            <li>
              Application subscription/DASH client dispatch of ISO BMFF Timed
              Metadata tracks providing similar functionality to DASH event streams.
            </li>
          </ul>
        </p>
        <p>
          Two modes for dispatching events are defined. In Mode 1, events are dispatched
          at the time the event arrives, and in Mode 2, events are dispatched at the
          given time on the media timeline. The "arrival" of events from the DASH client
          perspective may be either static or pre-provisioned, in the case MPD Events,
          or dynamic in the case of inband events carried in the <code>emsg</code>.
          The application can register with the DASH client which Mode to use.
        </p>
        <p>
          Reference: M&amp;E IG, Media Timed Events Task Force call 20 Aug 2018:
          <a href="https://www.w3.org/2018/08/20-me-minutes.html">Minutes</a>.
        </p>
      </section>
      <section>
        <h3>BBC Subtitle Guidelines</h3>
        <p>
          The BBC Subtitle Guidelines ([[BBC-SUBTITLES]]) describe best practice
          for authoring subtitles or captions. In particular, the guidelines state:
        </p>
        <blockquote>
          <p>
            <b>5.2 Match subtitle to pace of speaking</b>
          </p>
          <p>
            The subtitles should match the pace of speaking as closely
            as possible. Ideally, when the speaker is in shot, your subtitles
            should not anticipate speech by more than 1.5 seconds or hang up on
            the screen for more than 1.5 seconds after speech has stopped.
          </p>
          <p>
            <b>6.1 Match subtitles to shot</b>
          </p>
          <p>
            It is likely to be less tiring for the viewer if shot changes and
            subtitle changes occur at the same time. Many subtitles therefore
            start on the first frame of the shot and end on the last frame.
          </p>
          <p>
            <b>6.2 Maintain a minimum gap when mismatched</b>
          </p>
          <p>
            If you have to let a subtitle hang over a shot change, do not remove
            it too soon after the cut. The duration of the overhang will depend
            on the content.
          </p>
          <p>
            <b>6.3 Avoid straddling shot changes</b>
          </p>
          <p>
            Avoid creating subtitles that straddle a shot change (i.e., a subtitle
            that starts in the middle of shot one and ends in the middle of shot
            two). To do this, you may need to split a sentence at an appropriate
            point, or delay the start of a new sentence to coincide with the shot
            change.
          </p>
        </blockquote>
        <p>
          To meet these requirements, the playback system must honour the provided
          timings. Subtitles for video are typically authored against video at a
          nominal frame rate, e.g., 25 frames per second (which corresponds to
          40 milliseconds per frame). The actual video frame rate may be adjusted
          dynamically according to the video encoding, but the subtitle timing
          must remain the same ([[EBU-TT-D]], Annex E).
        </p>
      </section>
      <section>
        <h3>MPEG Working Draft on Carriage of Web Resources in ISO BMFF</h3>
        <p>
          [[WEB-ISOBMFF]] is a draft document that specifies the use of ISO BMFF
          tools for the storage and delivery of web data. The specified storage
          is designed to enable enriching audio/video content, as well as
          audio-only content, with synchronized, animated, interactive web data,
          including overlays.
        </p>
      </section>
      <section>
        <h3>WebVTT</h3>
        <p>
          [[WEBVTT]] is a W3C specification that provides a format for web video
          text tracks. A <code>VTTCue</code> is a text track cue, and may have
          attributes that affect rendering of the cue text on a web page.
          WebVTT metadata cues are text that is time-aligned.
        </p>
      </section>
    </section>
    <section>
      <h2>Gap analysis</h2>
      <p>
        This section describes gaps in existing existing Web platform
        capabilities needed to support the use cases and requirements described
        in this document. Where applicable, this section also describes how
        existing Web platform features can be used as workarounds, and any
        associated limitations.
      </p>
      <section>
        <h3>Synchronized event triggering</h3>
        <section>
          <h4>DASH and ISO BMFF emsg events</h4>
          <p>
            The <code>DataCue</code> API has been previously discussed as a means to
            deliver in-band event data to Web applications, but this is not implemented
            in mainstream browser engines. It is <a href="https://www.w3.org/TR/2018/WD-html53-20180426/semantics-embedded-content.html#text-tracks-exposing-inband-metadata">included</a>
            in the 26 April 2018 HTML 5.3 draft [[HTML53-20180426]], but is
            <a href="https://html.spec.whatwg.org/multipage/media.html#timed-text-tracks">not included</a>
            in [[HTML]]. See discussion <a href="https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/U06zrT2N-Xk">here</a>
            and notes on implementation status <a href="https://lists.w3.org/Archives/Public/public-html/2016Apr/0005.html">here</a>.
          </p>
          <p>
            Neither [[MSE-BYTE-STREAM-FORMAT-ISOBMFF]] nor [[INBANDTRACKS]] describe
            handling of <code>emsg</code> boxes.
          </p>
          <p>
            On resource constrained devices such as smart TVs and streaming sticks,
            parsing media segments to extract event information leads to a significant
            performance penalty, which can have an impact on UI rendering updates if
            this is done on the UI thread. There can also be an impact on the battery
            life of mobile devices. Given that the media segments will be parsed anyway
            by the user agent, parsing in JavaScript is an expensive overhead that
            could be avoided.
          </p>
          <p>
            [[HBBTV]] section 9.3.2 describes a mapping between the <code>emsg</code>
            fields described <a href="#mpeg-dash">above</a>
            and the <a href="https://html.spec.whatwg.org/multipage/media.html#texttrack"><code>TextTrack</code></a>
            and <a href="https://www.w3.org/TR/2018/WD-html53-20180426/semantics-embedded-content.html#datacue"><code>DataCue</code></a>
            APIs. A <code>TextTrack</code> instance is created for each event
            stream signalled in the MPD document (as identified by the
            <code>schemeIdUri</code> and <code>value</code>), and the
            <a href="https://html.spec.whatwg.org/multipage/media.html#dom-texttrack-inbandmetadatatrackdispatchtype"><code>inBandMetadataTrackDispatchType</code></a>
            <code>TextTrack</code> attribute contains the <code>scheme_id_uri</code>
            and <code>value</code> values. Because HbbTV devices include a native
            DASH client, parsing of the MPD document and creation of the
            <code>TextTrack</code>s is done by the UA.
          </p>
          <p class="ednote">
            To support DASH clients implemented in Web applications, there is
            therefore either a need for an API that allows applications to tell
            the UA which schemes it wants to receive, or the UA should simply
            expose all event streams to applications. Which of these is preferred?
          </p>
        </section>
        <section>
          <h4>Synchronization and timing</h4>
          <p>
            The timing guarantees provided in HTML5 regarding the triggering of
            <code>TextTrackCue</code> events may be not be enough to avoid
            <a href="https://lists.w3.org/Archives/Public/public-inbandtracks/2013Dec/0004.html">events being missed</a>.
          </p>
        </section>
      </section>
      <section>
        <h3>Synchronized rendering of web resources</h3>
        <p class="ednote">
          Describe gaps relating to synchronized rendering of web resources.
          Can we define a generic web API for scheduling page changes
          synchronized to playing media? Related: [[css-animations-1]],
          [[web-animations-1]], [[css-transitions-1]]. See also:
          <a href="https://github.com/bbc/VideoContext">https://github.com/bbc/VideoContext</a>.
          Should this be in scope for the TF?
        </p>
      </section>
      <section>
        <h3>Rendering of Web content embedded in media containers</h3>
        <p>
          There is no API for surfacing Web content embedded in ISO BMFF
          containers into the browser (e.g., the <code>HTMLCue</code> proposal
          discussed at <a href="https://www.w3.org/wiki/TPAC2015/HTMLcue">TPAC 2015</a>).
        </p>
        <p class="ednote">
          Add more detail on what's required. Some questions / considerations:
          <ul class="ednote">
            <li>Are the web resources intended to be handed to a Web application
            for rendering, or direct rendering by the UA?</li>
            <li>How do we guarantee that resources are delivered to the browser
            sufficiently ahead of time?</li>
            <li>How does same-origin policy affect such resources?</li>
          </ul>
        </p>
      </section>
    </section>
    <section>
      <h2>Recommendations</h2>
      <p class="ednote">
        Add recommendations here.
      </p>

      <p>
        To support DASH <code>emsg</code> and MPD events, we have the following
        requirements:
      </p>
      <p>
      <ul>
          <li>
            The API must allow Web applications to subscribe to receive media
            timed events with specific id and (optional) value.
          </li>
          <li>
            The user agent must deliver only those events to a Web application
            for which the application has subscribed.
          </li>
          <li>
            The API must allow a Web application to unsubscribe from receiving
            events with a specific id and (optional) value.
          </li>
          <li>
            To be able to handle out of band events, the API must allow Web
            applications to create events to be added to the media timeline,
            to be triggered by the user agent. Creating the event should allow
            the Web application to provide all necessary parameters to define
            the event, including start and end times, id and value, and data
            payload.
          </li>
          <li>
            For those events that the application has subscribed to receive,
            the API must:
            <ul>
              <li>
                Generate a JavaScript event when an in-band media timed event
                is parsed from the media container or media stream (DAInty Mode 1)
              </li>
              <li>
                Generate a JavaScript event when the current media playback
                position reaches the start time of a media timed event during
                playback (DAInty Mode 2)
              </li>
            </ul>
            This applies equally to in-band events that the user agent has extracted
            from the media container, and out-of-band events added by the Web application.
          </li>
          <li>
            The user agent must ensure that no events are missed during normal playback.
          </li>
        </ul>
      </p>
    </section>
    <section>
      <h2>Acknowledgments</h2>
      <p>
        Thanks to Charles Lo, Nigel Megitt, Jon Piesing, and Rob Smith for their
        contributions to this document.
      </p>
    </section>
  </body>
</html>
